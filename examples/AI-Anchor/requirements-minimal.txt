# AI Anchor 最小化依赖配置
# 解决vLLM和Ollama的依赖冲突问题

# ============ 基础依赖 ============
numpy>=1.24.0,<2.0.0  # vLLM严格要求

# Web框架
fastapi>=0.115.0
uvicorn[standard]>=0.32.0
websockets>=13.0

# 数据验证
pydantic>=2.7.0,<3.0.0
pydantic-settings>=2.6.0

# ============ AI模型核心 ============
# PyTorch基础
torch>=2.0.0
transformers>=4.45.0
accelerate>=1.0.0

# STT: vLLM (for Voxtral)
vllm>=0.6.0,<0.7.0

# LLM: Ollama
ollama>=0.4.0,<0.6.0

# ============ 音频处理 ============
soundfile>=0.12.0
librosa>=0.10.0
phonemizer>=3.0.0
# pyaudio - 请手动安装或使用系统包：apt install python3-pyaudio

# ============ 工具依赖 ============
python-dotenv>=1.0.0
loguru>=0.7.0
psutil>=6.0.0

# ============ 测试依赖 ============
pytest>=8.0.0
pytest-asyncio>=0.24.0